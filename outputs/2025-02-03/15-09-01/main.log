[2025-02-03 15:09:02,819][flwr][WARNING] - 
Setting `min_available_clients` lower than `min_fit_clients` or
`min_evaluate_clients` can cause the server to fail when there are too few clients
connected to the server. `min_available_clients` must be set to a value larger
than or equal to the values of `min_fit_clients` and `min_evaluate_clients`.

[2025-02-03 15:09:02,820][flwr][INFO] - Starting Flower simulation, config: num_rounds=5, no round_timeout
[2025-02-03 15:09:06,096][flwr][INFO] - Flower VCE: Ray initialized with resources: {'object_store_memory': 8763351859.0, 'memory': 17526703719.0, 'GPU': 1.0, 'accelerator_type:M4000': 1.0, 'node:10.40.177.142': 1.0, 'node:__internal_head__': 1.0, 'CPU': 16.0}
[2025-02-03 15:09:06,096][flwr][INFO] - Optimize your simulation with Flower VCE: https://flower.ai/docs/framework/how-to-run-simulations.html
[2025-02-03 15:09:06,096][flwr][INFO] - Flower VCE: Resources for each Virtual Client: {'num_cpus': 2, 'num_gpus': 0.1}
[2025-02-03 15:09:06,124][flwr][INFO] - Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
[2025-02-03 15:09:06,124][flwr][INFO] - [INIT]
[2025-02-03 15:09:06,124][flwr][INFO] - Requesting initial parameters from one random client
[2025-02-03 15:09:06,138][flwr][ERROR] - Traceback (most recent call last):
  File "/home/tt603/Desktop/federated_learning/Federated-Framework/.venv/lib/python3.8/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 88, in _submit_job
    self.actor_pool.submit_client_job(
  File "/home/tt603/Desktop/federated_learning/Federated-Framework/.venv/lib/python3.8/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 244, in submit_client_job
    self.submit(actor_fn, job)
  File "/home/tt603/Desktop/federated_learning/Federated-Framework/.venv/lib/python3.8/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 222, in submit
    future = fn(actor, app_fn, mssg, cid, context)
  File "/home/tt603/Desktop/federated_learning/Federated-Framework/.venv/lib/python3.8/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 89, in <lambda>
    lambda a, a_fn, mssg, partition_id, context: a.run.remote(
  File "/home/tt603/Desktop/federated_learning/Federated-Framework/.venv/lib/python3.8/site-packages/ray/actor.py", line 202, in remote
    return self._remote(args, kwargs)
  File "/home/tt603/Desktop/federated_learning/Federated-Framework/.venv/lib/python3.8/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/tt603/Desktop/federated_learning/Federated-Framework/.venv/lib/python3.8/site-packages/ray/util/tracing/tracing_helper.py", line 426, in _start_span
    return method(self, args, kwargs, *_args, **_kwargs)
  File "/home/tt603/Desktop/federated_learning/Federated-Framework/.venv/lib/python3.8/site-packages/ray/actor.py", line 327, in _remote
    return invocation(args, kwargs)
  File "/home/tt603/Desktop/federated_learning/Federated-Framework/.venv/lib/python3.8/site-packages/ray/actor.py", line 308, in invocation
    return actor._actor_method_call(
  File "/home/tt603/Desktop/federated_learning/Federated-Framework/.venv/lib/python3.8/site-packages/ray/actor.py", line 1440, in _actor_method_call
    object_refs = worker.core_worker.submit_actor_task(
  File "python/ray/_raylet.pyx", line 4180, in ray._raylet.CoreWorker.submit_actor_task
  File "python/ray/_raylet.pyx", line 4185, in ray._raylet.CoreWorker.submit_actor_task
  File "python/ray/_raylet.pyx", line 864, in ray._raylet.prepare_args_and_increment_put_refs
  File "python/ray/_raylet.pyx", line 855, in ray._raylet.prepare_args_and_increment_put_refs
  File "python/ray/_raylet.pyx", line 902, in ray._raylet.prepare_args_internal
  File "/home/tt603/Desktop/federated_learning/Federated-Framework/.venv/lib/python3.8/site-packages/ray/_private/serialization.py", line 494, in serialize
    return self._serialize_to_msgpack(value)
  File "/home/tt603/Desktop/federated_learning/Federated-Framework/.venv/lib/python3.8/site-packages/ray/_private/serialization.py", line 472, in _serialize_to_msgpack
    pickle5_serialized_object = self._serialize_to_pickle5(
  File "/home/tt603/Desktop/federated_learning/Federated-Framework/.venv/lib/python3.8/site-packages/ray/_private/serialization.py", line 425, in _serialize_to_pickle5
    raise e
  File "/home/tt603/Desktop/federated_learning/Federated-Framework/.venv/lib/python3.8/site-packages/ray/_private/serialization.py", line 420, in _serialize_to_pickle5
    inband = pickle.dumps(
  File "/home/tt603/Desktop/federated_learning/Federated-Framework/.venv/lib/python3.8/site-packages/ray/cloudpickle/cloudpickle.py", line 1479, in dumps
    cp.dump(obj)
  File "/home/tt603/Desktop/federated_learning/Federated-Framework/.venv/lib/python3.8/site-packages/ray/cloudpickle/cloudpickle.py", line 1245, in dump
    return super().dump(obj)
  File "/home/tt603/Desktop/federated_learning/Federated-Framework/.venv/lib/python3.8/site-packages/tensorflow/python/framework/ops.py", line 1061, in __reduce__
    return convert_to_tensor, (self._numpy(),)
  File "/home/tt603/Desktop/federated_learning/Federated-Framework/.venv/lib/python3.8/site-packages/tensorflow/python/framework/ops.py", line 1109, in _numpy
    raise core._status_to_exception(e) from None  # pylint: disable=protected-access
tensorflow.python.framework.errors_impl.InvalidArgumentError: Cannot convert a Tensor of dtype variant to a NumPy array.

[2025-02-03 15:09:06,138][flwr][ERROR] - Cannot convert a Tensor of dtype variant to a NumPy array.
[2025-02-03 15:09:06,139][flwr][ERROR] - Cannot convert a Tensor of dtype variant to a NumPy array.
[2025-02-03 15:09:06,140][flwr][ERROR] - Traceback (most recent call last):
  File "/home/tt603/Desktop/federated_learning/Federated-Framework/.venv/lib/python3.8/site-packages/flwr/simulation/app.py", line 339, in start_simulation
    hist = run_fl(
  File "/home/tt603/Desktop/federated_learning/Federated-Framework/.venv/lib/python3.8/site-packages/flwr/server/server.py", line 492, in run_fl
    hist, elapsed_time = server.fit(
  File "/home/tt603/Desktop/federated_learning/Federated-Framework/.venv/lib/python3.8/site-packages/flwr/server/server.py", line 93, in fit
    self.parameters = self._get_initial_parameters(server_round=0, timeout=timeout)
  File "/home/tt603/Desktop/federated_learning/Federated-Framework/.venv/lib/python3.8/site-packages/flwr/server/server.py", line 284, in _get_initial_parameters
    get_parameters_res = random_client.get_parameters(
  File "/home/tt603/Desktop/federated_learning/Federated-Framework/.venv/lib/python3.8/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 168, in get_parameters
    message_out = self._submit_job(message, timeout)
  File "/home/tt603/Desktop/federated_learning/Federated-Framework/.venv/lib/python3.8/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 108, in _submit_job
    raise ex
  File "/home/tt603/Desktop/federated_learning/Federated-Framework/.venv/lib/python3.8/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 88, in _submit_job
    self.actor_pool.submit_client_job(
  File "/home/tt603/Desktop/federated_learning/Federated-Framework/.venv/lib/python3.8/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 244, in submit_client_job
    self.submit(actor_fn, job)
  File "/home/tt603/Desktop/federated_learning/Federated-Framework/.venv/lib/python3.8/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 222, in submit
    future = fn(actor, app_fn, mssg, cid, context)
  File "/home/tt603/Desktop/federated_learning/Federated-Framework/.venv/lib/python3.8/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 89, in <lambda>
    lambda a, a_fn, mssg, partition_id, context: a.run.remote(
  File "/home/tt603/Desktop/federated_learning/Federated-Framework/.venv/lib/python3.8/site-packages/ray/actor.py", line 202, in remote
    return self._remote(args, kwargs)
  File "/home/tt603/Desktop/federated_learning/Federated-Framework/.venv/lib/python3.8/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/tt603/Desktop/federated_learning/Federated-Framework/.venv/lib/python3.8/site-packages/ray/util/tracing/tracing_helper.py", line 426, in _start_span
    return method(self, args, kwargs, *_args, **_kwargs)
  File "/home/tt603/Desktop/federated_learning/Federated-Framework/.venv/lib/python3.8/site-packages/ray/actor.py", line 327, in _remote
    return invocation(args, kwargs)
  File "/home/tt603/Desktop/federated_learning/Federated-Framework/.venv/lib/python3.8/site-packages/ray/actor.py", line 308, in invocation
    return actor._actor_method_call(
  File "/home/tt603/Desktop/federated_learning/Federated-Framework/.venv/lib/python3.8/site-packages/ray/actor.py", line 1440, in _actor_method_call
    object_refs = worker.core_worker.submit_actor_task(
  File "python/ray/_raylet.pyx", line 4180, in ray._raylet.CoreWorker.submit_actor_task
  File "python/ray/_raylet.pyx", line 4185, in ray._raylet.CoreWorker.submit_actor_task
  File "python/ray/_raylet.pyx", line 864, in ray._raylet.prepare_args_and_increment_put_refs
  File "python/ray/_raylet.pyx", line 855, in ray._raylet.prepare_args_and_increment_put_refs
  File "python/ray/_raylet.pyx", line 902, in ray._raylet.prepare_args_internal
  File "/home/tt603/Desktop/federated_learning/Federated-Framework/.venv/lib/python3.8/site-packages/ray/_private/serialization.py", line 494, in serialize
    return self._serialize_to_msgpack(value)
  File "/home/tt603/Desktop/federated_learning/Federated-Framework/.venv/lib/python3.8/site-packages/ray/_private/serialization.py", line 472, in _serialize_to_msgpack
    pickle5_serialized_object = self._serialize_to_pickle5(
  File "/home/tt603/Desktop/federated_learning/Federated-Framework/.venv/lib/python3.8/site-packages/ray/_private/serialization.py", line 425, in _serialize_to_pickle5
    raise e
  File "/home/tt603/Desktop/federated_learning/Federated-Framework/.venv/lib/python3.8/site-packages/ray/_private/serialization.py", line 420, in _serialize_to_pickle5
    inband = pickle.dumps(
  File "/home/tt603/Desktop/federated_learning/Federated-Framework/.venv/lib/python3.8/site-packages/ray/cloudpickle/cloudpickle.py", line 1479, in dumps
    cp.dump(obj)
  File "/home/tt603/Desktop/federated_learning/Federated-Framework/.venv/lib/python3.8/site-packages/ray/cloudpickle/cloudpickle.py", line 1245, in dump
    return super().dump(obj)
  File "/home/tt603/Desktop/federated_learning/Federated-Framework/.venv/lib/python3.8/site-packages/tensorflow/python/framework/ops.py", line 1061, in __reduce__
    return convert_to_tensor, (self._numpy(),)
  File "/home/tt603/Desktop/federated_learning/Federated-Framework/.venv/lib/python3.8/site-packages/tensorflow/python/framework/ops.py", line 1109, in _numpy
    raise core._status_to_exception(e) from None  # pylint: disable=protected-access
tensorflow.python.framework.errors_impl.InvalidArgumentError: Cannot convert a Tensor of dtype variant to a NumPy array.

[2025-02-03 15:09:06,140][flwr][ERROR] - Your simulation crashed :(. This could be because of several reasons. The most common are: 
	 > Sometimes, issues in the simulation code itself can cause crashes. It's always a good idea to double-check your code for any potential bugs or inconsistencies that might be contributing to the problem. For example: 
		 - You might be using a class attribute in your clients that hasn't been defined.
		 - There could be an incorrect method call to a 3rd party library (e.g., PyTorch).
		 - The return types of methods in your clients/strategies might be incorrect.
	 > Your system couldn't fit a single VirtualClient: try lowering `client_resources`.
	 > All the actors in your pool crashed. This could be because: 
		 - You clients hit an out-of-memory (OOM) error and actors couldn't recover from it. Try launching your simulation with more generous `client_resources` setting (i.e. it seems {'num_cpus': 2, 'num_gpus': 0.1} is not enough for your run). Use fewer concurrent actors. 
		 - You were running a multi-node simulation and all worker nodes disconnected. The head node might still be alive but cannot accommodate any actor with resources: {'num_cpus': 2, 'num_gpus': 0.1}.
Take a look at the Flower simulation examples for guidance <https://flower.ai/docs/framework/how-to-run-simulations.html>.
